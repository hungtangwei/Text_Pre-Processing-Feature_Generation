{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Talbe of contents\n",
    "\n",
    "* [Introduction](#introduction)\n",
    "* [PART A: Generate a sparse representation for Paper Bodies](#part_a)\n",
    "    * [Step 01: Import libraries](#step01)\n",
    "    * [Step 02: Convert pdf file to txt file](#step02)\n",
    "    * [Step 03: Download the PDF files of all the papers and translate to txt file](#step03)\n",
    "    * [Step 04: Create context-independent stop word list](#step04)\n",
    "    * [Step 05: Sparse feature generation](#step05)\n",
    "        * [5.1 Fisrt time text perporcessing on Paper Bodies](#step05-1)\n",
    "        * [5.2 Find the top 200 meaningful bigrams](#step05-2)\n",
    "        * [5.3 Find the context-dependent (with the threshold set to %95) stop words](#step05-3)\n",
    "        * [5.4 Find the Rare tokens (with the threshold set to 3%)](#step05-4)\n",
    "        * [5.5 Create all remove list](#step05-5)\n",
    "        * [5.6 Final text perporcessing on Paper Bodies](#step05-6)\n",
    "    * [Step 06: Create vocab.txt file](#step06)\n",
    "    * [Step 07: Create count_vectors.txt](#step07)\n",
    "* [PART B: Generate a CSV file (stats.csv) containing three columns](#part_b)\n",
    "    * [Step 01: Define functions to extract information from files.](#step1)\n",
    "    * [Step 02: Parse one pdf file in pdf_list](#step2)\n",
    "    * [Step 03: Word Tokenization](#step3)\n",
    "    * [Step 04: Top 10 Terms](#step4)\n",
    "    * [Step 05: Create a DataFrame and Generate a CSV file](#step5)\n",
    "* [Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "<a id=\"introduction\"></a>\n",
    "In this assignment, we extracting data from non-structured format (PDF file) and convert to the txt file, and extract data into a proper format suitable for a downstream modelling task. Finally, we convert them into numerical representations. \n",
    "Therefore, in this case, we will use some libraries, such as pdfminer and nltk we learned in tutorial, to finish this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART A:  Generate a sparse representation for Paper Bodies\n",
    "<a id=\"part_a\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 01.  Import libraries\n",
    "<a id=\"step01\"></a>\n",
    "\n",
    "Import some libraries for this assignment:\n",
    "* pandas (for dataframe, included in Anaconda Python 3.7) \n",
    "* re (for regular expression, included in Anaconda Python 3.7)\n",
    "* os (for pdf file to txt file)\n",
    "* pdfminer.six \n",
    "* pdfminer\n",
    "* nltk\n",
    "* requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to import libraries as you need in this assessment, e.g.,\n",
    "#! pip install pandas\n",
    "#! pip install nltk\n",
    "#! pip install pdfminer.six\n",
    "#! pip install requests\n",
    "#nltk.download('stopwords')\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO, open\n",
    "from nltk.collocations import *\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.probability import *\n",
    "porter_stem=PorterStemmer()\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 02. Convert pdf file to txt file\n",
    "<a id=\"step02\"></a>\n",
    "To convert the downloaded PDF file to a text file, we are going to use *pdf2txt.py*, a command that comes with pdfminer.\n",
    "\n",
    "Now we have pdfminer installed and are ready to convert our PDF to text by running the following command:\n",
    "```shell\n",
    "    pdf2txt.py -o Group081.txt Group081.pdf\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output='Group081'\n",
    "filename='Group081'\n",
    "os.system(\"pdf2txt.py -o {}.txt {}.pdf\".format(output, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 03. Download the PDF files of all the papers and translate to txt file\n",
    "<a id=\"step03\"></a>\n",
    "\n",
    "In this steop, we do the following things:\n",
    "* Use regular expression to extract the paper id and download url.\n",
    "```shell\n",
    "    Paper ID regular expression: r\"(.*?)\\.pdf\"\n",
    "    Paper download url regular expression: r\"\\.pdf (http.*)\"\n",
    "```\n",
    "* Use requests library to programmatically download papers from url.\n",
    "\n",
    "* Download PDF files and use pdf2txt.py to convert PDF to txt file. Therefore, we will have 200 PDF files and 200 txt files. \n",
    "\n",
    "* Create a paper_id variable of a list that store the ID of these 200 papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input the file\n",
    "pdfTxtFile = 'Group081.txt'\n",
    "pdf_txt = open(pdfTxtFile, 'r')\n",
    "\n",
    "pattern_url = re.compile(r'\\.pdf (http.*)')\n",
    "pattern_id = re.compile(r'(.*?)\\.pdf')\n",
    "paper_id=[]\n",
    "\n",
    "#read each line in file, extract the paper id and url, and append id to paper_id list\n",
    "#use requests to download the url file, and convert pdf file to txt file by using pdf2txt.py\n",
    "for line in pdf_txt:\n",
    "    match_url=pattern_url.search(line)\n",
    "    match_id=pattern_id.search(line)\n",
    "    if match_url and match_id is not None:\n",
    "        url=match_url.group(1)\n",
    "        r = requests.get(url)\n",
    "        url_id=match_id.group(1)\n",
    "        url_id_pdf=url_id+'.pdf'\n",
    "        paper_id.append(url_id)\n",
    "        with open(url_id_pdf, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "            os.system(\"pdf2txt.py -o {}.txt {}.pdf\".format(url_id, url_id))\n",
    "            \n",
    "#close the file            \n",
    "pdf_txt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 04. Create context-independent stop word list\n",
    "<a id=\"step04\"></a>\n",
    "\n",
    "In this step, the context-independent stop words list we create in this case is from stopwords_en.txt provided in the zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input the file\n",
    "stop_word_txt=open('stopwords_en.txt','r')\n",
    "context_independent_stopwords=[]\n",
    "\n",
    "#read each line in file, strip each line and append stop word to stop_word list\n",
    "for word in stop_word_txt:\n",
    "    a=word.strip()\n",
    "    context_independent_stopwords.append(a)\n",
    "\n",
    "#close the file\n",
    "stop_word_txt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 05. Sparse feature generation \n",
    "<a id=\"step05\"></a>\n",
    "In this assignment, we will do perform text preprocessing on Paper Bodies to sparse feature feneration two times.\n",
    "* The first time we perform text preprocessing on paper bodies is to find:\n",
    "    * First 200 meaningful bigrams\n",
    "    * Context-dependent (with the threshold set to %95) stop words\n",
    "    * Rare tokens (with the threshold set to 3%)\n",
    "\n",
    "\n",
    "* The Second time we perform text preprocessing on paper bodies is to sparse feature generation:\n",
    "    * Convert the Tokens to bigram if these two tokens are in top 200 meaningful bigrams.\n",
    "    * remove all the stop words and Rare tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5-1. First time text perporcessing on Paper Bodies\n",
    "<a id=\"step05-1\"></a>\n",
    "In this first time text perporcessing on paper body, we will do following things to find the first 200 meaningful biframs, context-dependent stop words, and rare tokens:\n",
    "* 1. Convert the txt file to one row.\n",
    "* 2. Extract the Paper bodies in each paper.\n",
    "* 3. Lowercase the first word in each sentence in paper body. (E)\n",
    "* 4. Word tokenization by using the following regular expression, r\"[A-Za-z]\\w+(?:[- '?]\\w+)?\"      (A)\n",
    "* 5. Remove context-independent stop words.      (B)\n",
    "* 6. Remove tokens with the length less than 3      (F)\n",
    "* 7. Extracting 2-grams to bigrams_raw list.\n",
    "* 8. append token to first_step that the key is paper id, and the value is token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "   Create the to_one_row function that convert the txt file to one row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_row(txt_file):   \n",
    "    txt=[]\n",
    "    #read each line in txt_file, and replace '\\n' to ''\n",
    "    for rows in txt_file:\n",
    "        row_replaced=rows.replace('\\n','')\n",
    "        # if the length of row_replaced not equal to 0, append to txt\n",
    "        if len(row_replaced) != 0:\n",
    "            txt.append(row_replaced)\n",
    "    \n",
    "    # join to one row of string.\n",
    "    txt_string=' '.join(txt)\n",
    "    return txt_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "   Create the first_word_lowercase function that lowercase the first word in each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_word_lowercase(sentence):\n",
    "    low_sentence=[]\n",
    "    #read each line in sentence, and change the first character to lower case\n",
    "    for rows in sentence:\n",
    "        row=rows.strip()\n",
    "        low_sent=row[0].lower() + row[1:]\n",
    "        low_sentence.append(low_sent)\n",
    "    return low_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stem=PorterStemmer()\n",
    "tokenizer = RegexpTokenizer(r\"[A-Za-z]\\w+(?:[-'?]\\w+)?\")\n",
    "\n",
    "first_step={}\n",
    "bigrams_raw=[]\n",
    "\n",
    "# read each line in paper_id\n",
    "for ID in paper_id:\n",
    "    pdfTxtFile = ID+'.txt'\n",
    "    pdf_txt = open(pdfTxtFile, 'r')\n",
    "    \n",
    "    # use to_one_row function to convert sentence to one row\n",
    "    txt_string=to_one_row(pdf_txt)\n",
    "    \n",
    "    # extract the paper body\n",
    "    pattern_context=re.compile(r'1 Paper Body(.*)2 References')\n",
    "    context=pattern_context.search(txt_string).group(1)\n",
    "    \n",
    "    #use sent_tokenize to sentence segmentation.\n",
    "    sent_token=sent_tokenize(context)\n",
    "        \n",
    "    #use first_word_lowercase to lower the fisrt word in sentence segmentation sentence.\n",
    "    low_sentence=first_word_lowercase(sent_token)\n",
    "    \n",
    "    sentence_token = []\n",
    "\n",
    "    #read each line in low_sentence\n",
    "    for sent in low_sentence:\n",
    "        #tokenize the word\n",
    "        unigram_tokens = tokenizer.tokenize(sent)\n",
    "        \n",
    "        #remove context-independent stop word and token less than 3\n",
    "        remove_stopword_uni=[word for word in unigram_tokens if word.lower() not in context_independent_stopwords and len(word)>2]\n",
    "\n",
    "        #append token to bigrams_list\n",
    "        bigrams_list=[word for word in remove_stopword_uni]\n",
    "        \n",
    "        # append each token to sentenc_token list\n",
    "        for uni in remove_stopword_uni:\n",
    "            sentence_token.append(uni)\n",
    "        \n",
    "        # use ngrams function to make 2-grams in each sentence\n",
    "        bigrams_f = ngrams(bigrams_list, n = 2)\n",
    "        \n",
    "        # append bigram to bigrams_raw list\n",
    "        for bigram in bigrams_f:\n",
    "            bigrams_raw.append(bigram)\n",
    "\n",
    "    #append token to first_step that the key is paper id, and the value is sentence_token.\n",
    "    first_step[ID]=list(set(sentence_token))\n",
    "    \n",
    "    pdf_txt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5-2. Find the top 200 meaningful bigrams\n",
    "<a id=\"step05-2\"></a>\n",
    "In this step we use FreqDist to find the top 200 meaningful bigrams and separated meaningful bigrams using double underscore i.e. “__”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use FreqDist function to count the bigrams\n",
    "fd_bigram = FreqDist(bigrams_raw)\n",
    "#find the top 200 meaning bigrams\n",
    "bigram_200=fd_bigram.most_common(200)\n",
    "\n",
    "bigram_list=[]\n",
    "bigram_find=[]\n",
    "\n",
    "#read each line in bigram_200, and use double underscore to join\n",
    "for i in bigram_200:\n",
    "    a='__'.join(i[0])\n",
    "    bigram_list.append(a)\n",
    "    bigram_find.append(i[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5-3. Find the context-dependent (with the threshold set to %95) stop words\n",
    "<a id=\"step05-3\"></a>\n",
    "\n",
    "In this step, we use count variable to count the number. If the word appears in one paper, the count will plus one. Finally, if the count better than 190, we will append this word to context-dependent stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_dependent_stopwords=[]\n",
    "# read each word_list in first_stop.values\n",
    "for word_list in first_step.values():    \n",
    "    #read each word in word_list\n",
    "    for word in word_list:\n",
    "        count=0\n",
    "        #read each paper id in paper_id\n",
    "        for Id in paper_id:\n",
    "            # if word appears in each paper values, count plus one\n",
    "            if word in first_step[Id]:\n",
    "                count +=1\n",
    "        #if count better than 190, and the word not in context_dependent_stopwords, append this word to context_dependent_stopwords list\n",
    "        if count>190 and word not in context_dependent_stopwords:\n",
    "            context_dependent_stopwords.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5-4. Find the Rare tokens (with the threshold set to 3%)\n",
    "<a id=\"step05-4\"></a>\n",
    "\n",
    "In this step, we use count variable to count the number. If the word appears in one paper, the count will plus one. Finally, if the count less than 7, we will append this word to rare token list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_tokens=[]\n",
    "# read each word_list in first_stop.values\n",
    "for word_list in first_step.values():\n",
    "    #read each word in word_list\n",
    "    for word in word_list:\n",
    "        count=0\n",
    "        #read each paper id in paper_id\n",
    "        for Id in paper_id:\n",
    "            # if word appears in each paper values, count plus one\n",
    "            if word in first_step[Id]:\n",
    "                count +=1\n",
    "        #if count less than 7, and the word not in rare_tokens, append this word to rare_tokens list\n",
    "        if count<7 and word not in rare_tokens:   \n",
    "            rare_tokens.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5-5. Create all remove list\n",
    "<a id=\"step05-5\"></a>\n",
    "\n",
    "In this step, we create all remove list by combining context-independent and context-dependent stop words and rare token list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_remove_list=context_independent_stopwords+context_dependent_stopwords + rare_tokens "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5-6. Final text perporcessing on Paper Bodies\n",
    "<a id=\"step05-6\"></a>\n",
    "\n",
    "In this first time text perporcessing on paper body, we will do following things:\n",
    "(E->A->G->B->D->F->C)\n",
    "* 1. Convert the txt file to one row.\n",
    "* 2. Extract the Paper bodies in each paper.\n",
    "* 3. Lowercase the first word in each sentence in paper body. (E)\n",
    "* 4. Word tokenization by using the following regular expression, r\"[A-Za-z]\\w+(?:[- '?]\\w+)?\"(A)\n",
    "* 5. Replace token to bigram if this token is in the top 200 meaningful bigrams list (G)\n",
    "* 6. Remove context-independent and context-dependent stop words   (B)\n",
    "* 7. Remove Rare tokens in 3% Rare tokens list (D) \n",
    "* 8. Remove tokens with the length less than 3 (F)\n",
    "* 9. Stem token by using the Porter stemmer. (C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.probability import *\n",
    "porter_stem=PorterStemmer()\n",
    "\n",
    "final={}\n",
    "\n",
    "tokenizer = RegexpTokenizer(r\"[A-Za-z]\\w+(?:[-'?]\\w+)?\")\n",
    "lookup=set(bigram_find)\n",
    "\n",
    "# read each id in paper_id\n",
    "for ID in paper_id:\n",
    "    pdfTxtFile = ID+'.txt'\n",
    "    pdf_txt = open(pdfTxtFile, 'r')\n",
    "\n",
    "    # use to_one_row function to convert sentence to one row\n",
    "    txt_string=to_one_row(pdf_txt)    \n",
    "    \n",
    "    # extract the paper body\n",
    "    pattern_context=re.compile(r'1 Paper Body(.*)2 References')\n",
    "    context=pattern_context.search(txt_string).group(1)\n",
    "    \n",
    "    #use sent_tokenize to sentence segmentation.\n",
    "    sent_token=sent_tokenize(context)\n",
    "\n",
    "    #use first_word_lowercase to lower the fisrt word in sentence segmentation sentence.\n",
    "    low_sentence=first_word_lowercase(sent_token)\n",
    "    \n",
    "    uni_token = []\n",
    "    \n",
    "    #read each line in low_sentence\n",
    "    for sent in low_sentence:\n",
    "        #tokenize the word\n",
    "        unigram_tokens = tokenizer.tokenize(sent)\n",
    "        \n",
    "        #remove context-independent stop word and token less than 3\n",
    "        remove_stopword_sent=[word for word in unigram_tokens if word.lower() not in context_independent_stopwords]\n",
    "        \n",
    "        for uni in remove_stopword_sent:\n",
    "            uni_token.append(uni)\n",
    "    \n",
    "    bigramed_uni = []\n",
    "        \n",
    "    #Replace token to bigram if this token is in the top 200 meaningful bigrams list\n",
    "    word_iter = iter(range(len(uni_token)))\n",
    "    for index in word_iter:\n",
    "        bigramed_uni.append(uni_token[index])\n",
    "        if index < (len(uni_token) - 1) and (uni_token[index], uni_token[index+1]) in lookup:\n",
    "            bigramed_uni[-1] =bigramed_uni[-1] +'__'+uni_token[index+1]\n",
    "            next(word_iter)\n",
    "    \n",
    "    \n",
    "    stemmed_uni=[]\n",
    "    \n",
    "    # read each token in bigramed_uni\n",
    "    for uni in bigramed_uni:\n",
    "        #if token is upper case, length of token is greater than 2, and token is not in all_remove_list, append token to stemmed_uni list\n",
    "        if uni[0].isupper() and len(uni)>2 and uni not in all_remove_list:\n",
    "            stemmed_uni.append(uni)\n",
    "        # if the length of token is greater than 2, and token is not in all_remove_list, use porter stemming to stem the token, and append to stemmed_uni list\n",
    "        elif len(uni)>2 and uni not in all_remove_list:\n",
    "            stem_uni=porter_stem.stem(uni)\n",
    "            stemmed_uni.append(stem_uni)\n",
    "            \n",
    "    #append token to final dictionary that the key is paper id, and the value is stemmed_uni.\n",
    "    final[ID]=stemmed_uni\n",
    "    \n",
    "    pdf_txt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 06. Create vocab.txt file\n",
    "<a id=\"step06\"></a>\n",
    "In this step, vocab.txt file contains the bigrams and unigrams tokens in the following format, token_string:token_index. We sort the Words in the vocabulary by using alphabetical ascending order. Finally convert to the txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input the file\n",
    "f=open('Group081_vocab.txt','w')\n",
    "\n",
    "#append unique word to word_list\n",
    "vocab=[]\n",
    "for word_list in final.values():\n",
    "    for word in word_list:\n",
    "        if word not in vocab :\n",
    "            vocab.append(word)           \n",
    "\n",
    "# to sort the word_list\n",
    "sorted_list = sorted(vocab)\n",
    "\n",
    "final_output={}\n",
    "\n",
    "#use enumerate to read index and word in sorted_list, and write into Group081_vocab.txt file\n",
    "for index, word in enumerate(sorted_list):\n",
    "    final_output[word]=index\n",
    "    f.write(word+':'+str(index)+'\\n')\n",
    "\n",
    "#close the file    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 07. Create count_vectors.txt\n",
    "<a id=\"step07\"></a>\n",
    "In this step, Each line in the txt file contains the sparse representations of one of the papers in the following format: (paper_id, token1_index:token1_wordcount, token2_index:token2_wordcount ...) Therefore, we will do following steps:\n",
    "* Translate the token_string to token_index in each paper\n",
    "* Use FreqDist to count the token_index in each paper\n",
    "* Write to count_vectors.txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_value={}\n",
    "# read each id in paper_id\n",
    "for ID in paper_id:\n",
    "    temp=[]\n",
    "    # read each word in final[ID], and convert the token_string to token_index\n",
    "    for word in final[ID]:\n",
    "        temp.append(final_output[word])\n",
    "    \n",
    "    #append token_index to final_value dictionary that the key is paper id, and the value is token_index list.\n",
    "    final_value[ID]=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import *\n",
    "\n",
    "#open the file\n",
    "f1=open('Group081_count_vectors.txt','w')\n",
    "\n",
    "# read key and value in final_value dictionary\n",
    "for key,value in final_value.items():\n",
    "    #use FreqDist function to count the frequence of token_index, and write to Group081_count_vectors.txt file\n",
    "    fd_word_index = FreqDist(value)\n",
    "    word_index_count=fd_word_index.most_common()\n",
    "    f1.write(key+',')\n",
    "    for i in word_index_count:   \n",
    "        f1.write(str(i[0])+':'+str(i[1])+',')\n",
    "    f1.write('\\n')    \n",
    "\n",
    "#close the file    \n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART B: Generate a CSV file (stats.csv) containing three columns\n",
    "<a id=\"part_b\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 01. Define functions to extract information from files and sort lists.\n",
    "<a id=\"step1\"></a>\n",
    "Because we need to extract information from 200 files in a for loop, so using fuctions is an easier and more efficient method.\n",
    "\n",
    "For abstract, information is between \"Abstract\" and \"1 Paper Body\", so its pattern is **\"Abstract(.+?)1 Paper Body\"**. In the meantime, abstract must be normalized to lowercase except the capital tokens appearing in the middle of a sentence/line. So we use sentence segmentation to split lines into sentences, and lowercase the first letter of a sentence. Finally, we transform sentences into a string again.\n",
    "\n",
    "For title, information is the lines before \"Authored by:\", so its pattern is **\"(.+?)Authored by:\"**. After extract, we lowercase titles.\n",
    "\n",
    "For author, information is between \"Authored by:\" and \"Abstract\", so its pattern is **\"Authored by:\\s\\*(.+?)\\s\\*Abstract\"**. The result returned is a list.\n",
    "\n",
    "For sort list, we used bubble sort here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to extract abstract\n",
    "def extract_abstract(text):\n",
    "    abstract_pattern = re.compile(r\"Abstract(.+?)1 Paper Body\") # pattern for abstract\n",
    "    abstract = abstract_pattern.search(raw_text).group(1).strip() # delete \\n in the end\n",
    "    \n",
    "    # sentence segmentation\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle') \n",
    "    sentences = sent_detector.tokenize(abstract)\n",
    "    abstract_lower_list = []\n",
    "    for sent in sentences:\n",
    "        sent = sent[0].lower() + sent[1:] # lowercase the first letter of a sentence\n",
    "        abstract_lower_list.append(sent) # append to lower list\n",
    "    abstract_lower = ' '.join(abstract_lower_list) # combine to a string\n",
    "    return abstract_lower\n",
    "\n",
    "# define a function to extract title\n",
    "def extract_title(text):\n",
    "    title_pattern = re.compile(r\"(.+?)Authored by:\") # pattern for title\n",
    "    title = title_pattern.search(raw_text).group(1).strip().lower() # delete \\n in the end and lowercase\n",
    "    return title\n",
    "\n",
    "# define a function to extract author\n",
    "def extract_author(text):\n",
    "    author_pattern = re.compile(r\"Authored by:\\s*(.+?)\\s*Abstract\", re.DOTALL) # pattern for title and search in lines\n",
    "    author_group = author_pattern.search(raw_text).group(1).strip() # delete \\n in the end\n",
    "    author = author_group.split('\\n') # split by \\n into a list\n",
    "    return author\n",
    "\n",
    "# define bubble_sort to sort freqdist\n",
    "def bubble_sort(tuple_list):\n",
    "    n = len(tuple_list)\n",
    "    for i in range(n-1, 0, -1):\n",
    "        for j in range(i):\n",
    "            if tuple_list[j][1] == tuple_list[j+1][1]:\n",
    "                if tuple_list[j][0] > tuple_list[j+1][0]:\n",
    "                    tuple_list[j],tuple_list[j+1] = tuple_list[j+1],tuple_list[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize lists containing items required\n",
    "abstract_list = []\n",
    "title_list = []\n",
    "author_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize stopwords list and turn it into a set\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_set = set(stopwords_list)\n",
    "\n",
    "# add words in stopwords_en.txt into the set\n",
    "with open('stopwords_en.txt', 'r') as stopwords_en:\n",
    "    for word in stopwords_en:\n",
    "        stopwords_set.add(word.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step02. Parse one pdf file in pdf_list\n",
    "<a id=\"step2\"></a>\n",
    "Fistly, we concatenate lines in the file into one string without '\\n'. Then use functions to extract abstracts and titles and append them into lists respectively. After that, we concatenate lines into one string again, however, with '\\n' this time. Then we extract a list of authores. Finally, we append items which are not \"\" into the author list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open files in the pdf_list and process\n",
    "for ID in paper_id:\n",
    "    pdfTxtFile = ID+'.txt'\n",
    "    # concatenate lines into one string without '\\n'\n",
    "    with open(pdfTxtFile) as pdf_text:\n",
    "        raw_list = []\n",
    "        for lines in pdf_text:\n",
    "            raw_list.append(lines.strip()) # delete \\n in the end of the line\n",
    "            raw_text = \" \".join(raw_list)\n",
    "\n",
    "    # call functions and append into lists\n",
    "    abstract_list.append(extract_abstract(raw_text))\n",
    "    title_list.append(extract_title(raw_text))\n",
    "    \n",
    "    # open file and concatenate lines into one string with '\\n'\n",
    "    with open(pdfTxtFile) as pdf_text:\n",
    "        raw_list = []\n",
    "        for lines in pdf_text:\n",
    "            raw_list.append(lines)\n",
    "            raw_text = \"\".join(raw_list)\n",
    "    \n",
    "    # append author into the author_list\n",
    "    for author in extract_author(raw_text):\n",
    "        if author != '':\n",
    "            author_list.append(author)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 03. Word Tokenization\n",
    "<a id=\"step3\"></a>\n",
    "Firstly we set up a expression for tokenizer, then we use it in the two lists which are required tokeniztaion. After that, remove stopwords from them according to the stopwords set. Finally we import nltk.probability package to get the FreqDist of all three lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word tokenization\n",
    "tokenizer = RegexpTokenizer(r\"[A-Za-z]\\w+(?:[-'?]\\w+)?\")\n",
    "abstract_tokens = tokenizer.tokenize(' '.join(abstract_list))\n",
    "title_tokens = tokenizer.tokenize(' '.join(title_list))\n",
    "\n",
    "# remove stopwords\n",
    "stopped_abstract = [w for w in abstract_tokens if w not in stopwords_set]\n",
    "stopped_title = [w for w in title_tokens if w not in stopwords_set]\n",
    "\n",
    "# find freqidst for each list\n",
    "fd_abstract = FreqDist(stopped_abstract)\n",
    "fd_title = FreqDist(stopped_title)\n",
    "fd_author = FreqDist(author_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 04. Top 10 Terms\n",
    "<a id=\"step4\"></a>\n",
    "Use FreDist to get top 10 terms in each list by most_common function. And use bubble sort to sort the list based on alphabetical ascending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find freqdist for each list and find most_common 10\n",
    "fd_abstract = FreqDist(stopped_abstract).most_common(10)\n",
    "fd_title = FreqDist(stopped_title).most_common(10)\n",
    "fd_author = FreqDist(author_list).most_common(10)\n",
    "\n",
    "# sort lists by bubble sort\n",
    "bubble_sort(fd_abstract)\n",
    "bubble_sort(fd_title)\n",
    "bubble_sort(fd_author)\n",
    "\n",
    "# extract terms from lists\n",
    "top10_terms_in_abstracts = [item[0] for item in fd_abstract]\n",
    "top10_terms_in_titles = [item[0] for item in fd_title]\n",
    "top10_authors = [item[0] for item in fd_author]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 05. Create a DataFrame and Generate a CSV file\n",
    "<a id=\"step5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write lists into a dataframe\n",
    "df = pd.DataFrame({'top10_terms_in_abstracts': top10_terms_in_abstracts,\\\n",
    "                   'top10_terms_in_titles':top10_terms_in_titles,\\\n",
    "                   'top10_authors':top10_authors})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write dataframe into a csv file\n",
    "with open('Group081_stats.csv', 'w') as stats:\n",
    "    df.to_csv(\"Group081_stats.csv\",index=False,sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "<a id=\"summary\"></a>\n",
    "In this assignment,fisrst, we can understand that PDF is not a preferred storage or presentation format. Because, when we convert the pdf file to txt file, there are some convert mistake. However, sometimes we do not have any other choice. Second, in this case we need to figure out the correct order of operations that produces the correct set of vocabulary. The different order will affect different final result. In a sense, we are supposed to choose the most suitable order of steps according to tasks specifically."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
